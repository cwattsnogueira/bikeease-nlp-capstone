# -*- coding: utf-8 -*-
"""NaturalLanguageProcessingIncrementalCapstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N0cHR4SzFHh9n8htYrGDL8bRrleVffGc

# Natural Language Processing: Incremental Capstone

Input dataset:
https://drive.google.com/drive/folders/13-g3jxhPR0btN_s77KbcempcFXZ0RoqT

Carllos Watts-Nogueira

Due Aug 23 by 12:59am

Project Overview

The goal is to analyze thousands of customer reviews collected across platforms to:
- Automatically classify review sentiments (positive, neutral, negative)
- Extract underlying themes using topic modeling
- Help BikeEase respond better to user feedback and improve service

This transitions your role from purely predicting numbers (like rental counts) to understanding how people feel, using Natural Language Processing (NLP).

# Installing & importing packages
"""

# Install required packages (only once; skip if already installed)
# NLTK (Natural Language Toolkit) is a Python library for teaching and working with human language data, offering tools for tokenization, tagging, and parsing.
!pip install nltk

# Import necessary packages
import nltk
from nltk.tokenize import sent_tokenize

# import pandas
import pandas as pd

# Import Required Libraries
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

"""# Download necessary models from nltk"""

# Download required NLTK data
nltk.download('punkt')  # Sentence tokenizer
nltk.download('words')  # English word list
nltk.download('stopwords')  # Common stopwords
nltk.download('wordnet')  # WordNet lexical database
nltk.download('punkt_tab') # Download punkt_tab resource

"""# Import the necessary corpus"""

# Import WordNet, a lexical database used for lemmatization and semantic analysis
from nltk.corpus import wordnet

# Import a list of common English stopwords (e.g., "the", "and", "is") used for filtering out non-informative words
from nltk.corpus import stopwords

"""# Load your data and preprocess it"""

# Load dataset
df = pd.read_csv("bike_rental_reviews.csv")

df.head()

df.isnull().sum()

df.isna().sum()

"""# Change to lowercase / Removing punctuation / Word tokenization / Lemmatization / Remove stopwords"""

import re                # Import the regular expressions module for pattern-based text cleaning (e.g., removing punctuation) (regular expressions)
import string            # Import the string module to access common string constants like punctuation (punctuation and character sets)
from nltk.stem import WordNetLemmatizer  # lemmatizer class

# Define preprocessing a cleaning function --> clean_my_df
# Take the steps below and put them into a cleaning function, e.g. clean_my_df.
def clean_my_df(text):
    text = text.lower()
    text = re.sub(f"[{re.escape(string.punctuation)}]", "", text) #Removing punctuation
    tokens = nltk.word_tokenize(text) #Word tokenization
    lemmatizer = WordNetLemmatizer() #Lemmatization
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    stop_words = set(stopwords.words('english')) #Remove stopwords
    cleaned = [word for word in tokens if word not in stop_words]
    return " ".join(cleaned)

# Apply cleaning --> review_clean
# Apply your cleaning function to the DataFrame to create a new column, e.g. review_clean.
df["review_clean"] = df["review_text"].apply(clean_my_df)

# Encode sentiment labels
# Encode your sentiment labels into a new column sentiment_label.
sentiment_map = {'positive': 1, 'neutral': 0, 'negative': -1}
df["sentiment_label"] = df["sentiment"].map(sentiment_map)

df.head()

"""# Sentiment Analysis with TF-IDF, Naive Bayes, and Logistic Regression"""

# --------------------------------------------
# Final Step: Sentiment Analysis with TF-IDF, Naive Bayes, and Logistic Regression
# --------------------------------------------

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score

# Replace these with your actual column names!
text_column = df["review_clean"] #"your_clean_text"  # Column containing preprocessed review text
label_column = df["sentiment_label"] #"sentiment_label" # Column with sentiment labels (e.g., 1, 0, -1)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    df["review_clean"], df["sentiment_label"], test_size=0.2, random_state=42
)

# -------------------------------
# TF-IDF vectorization
# -------------------------------
tfidf = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

# -------------------------------
# Naive Bayes Model
# -------------------------------
nb = MultinomialNB()
nb.fit(X_train_tfidf, y_train)
nb_preds = nb.predict(X_test_tfidf)

print("Naive Bayes Performance:")
print("Accuracy:", accuracy_score(y_test, nb_preds))
print(classification_report(y_test, nb_preds))

# -------------------------------
# Logistic Regression Model
# -------------------------------
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train_tfidf, y_train)
logreg_preds = logreg.predict(X_test_tfidf)

print("Logistic Regression Performance:")
print("Accuracy:", accuracy_score(y_test, logreg_preds))
print(classification_report(y_test, logreg_preds))

# Confusion Matrix for Naive Bayes
nb_cm = confusion_matrix(y_test, nb_preds)

plt.figure(figsize=(6,4))
sns.heatmap(nb_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])
plt.title("Naive Bayes – Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Confusion Matrix for Logistic Regression
logreg_cm = confusion_matrix(y_test, logreg_preds)

plt.figure(figsize=(6,4))
sns.heatmap(logreg_cm, annot=True, fmt='d', cmap='Greens', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])
plt.title("Logistic Regression – Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""# Final Report - Analysis with TF-IDF, Naive Bayes, and Logistic Regression

### Confusion Matrix Comparison

| Model               | Negative | Neutral | Positive | Misclassifications |
|---------------------|----------|---------|----------|---------------------|
| **Naive Bayes**     | 3325     | 3317    | 3358     | 0                   |
| **Logistic Regression** | 3325     | 3317    | 3358     | 0                   |

Both models achieved **good?! classification** across all sentiment categories, with **100% accuracy**, **no misclassifications**, and **identical performance**.

---

### Insights

- **Data Quality**: The flawless results suggest the dataset is exceptionally clean and well-balanced. It may also indicate that the sentiment labels are strongly aligned with the textual features.
  
- **Model Behavior**: Both Naive Bayes and Logistic Regression performed optimally, likely due to the effectiveness of TF-IDF in separating sentiment classes. The feature space appears highly discriminative.

- **Risk of Overfitting**: While the metrics are impressive, such perfect scores raise concerns about potential overfitting or overly simplistic label-text relationships.

---

### Preprocessing Pipeline

To prepare the review text for modeling, I implemented a custom cleaning function using NLTK. The steps included:

- Lowercasing all text  
- Removing punctuation (`re` and `string`)  
- Tokenization (`nltk.word_tokenize`)  
- Lemmatization (`WordNetLemmatizer`)  
- Stopword removal (`nltk.corpus.stopwords`)  

The cleaned output was stored in a new column: `review_clean`.

---

### Label Encoding

Sentiment labels were mapped to numeric values to enable multiclass classification:

- **Positive** → `1`  
- **Neutral** → `0`  
- **Negative** → `-1`  

---

### Feature Extraction with TF-IDF

Using `TfidfVectorizer`, I transformed the cleaned text into numerical features. The `max_features` parameter was set to **5000** to retain the most informative terms while controlling dimensionality.

---

### Models Used

#### 1. **Multinomial Naive Bayes**
- Fast and efficient for text classification  
- Assumes conditional independence between words  
- Ideal for sparse, high-dimensional data like TF-IDF

#### 2. **Logistic Regression**
- Linear model capable of handling multiclass problems  
- Configured with `max_iter=1000` to ensure convergence  
- Robust and interpretable baseline for text classification

---

### Model Evaluation

Both models achieved:

- **Accuracy**: 1.00  
- **Precision, Recall, F1-score**: 1.00 across all classes

#### Classification Report:

| Class     | Precision | Recall | F1-score | Support |
|-----------|-----------|--------|----------|---------|
| Negative  | 1.00      | 1.00   | 1.00     | 3325    |
| Neutral   | 1.00      | 1.00   | 1.00     | 3317    |
| Positive  | 1.00      | 1.00   | 1.00     | 3358    |

---

### Reflections

This project deepened my understanding of:

- Building robust text preprocessing pipelines  
- Feature engineering with TF-IDF  
- Training and evaluating classification models  
- The importance of critically assessing good results

# Add a BERT, Text Blob, Vader, or Flair model here and compare to your above results!

## TextBlob – Lightweight and Easy
"""

from textblob import TextBlob

def get_textblob_sentiment(text):
    polarity = TextBlob(text).sentiment.polarity
    if polarity > 0.1:
        return 1  # positive
    elif polarity < -0.1:
        return -1  # negative
    else:
        return 0  # neutral

df["textblob_pred"] = df["review_text"].apply(get_textblob_sentiment)

"""## VADER – Rule-Based, Great for Social Text"""

from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

vader = SentimentIntensityAnalyzer()

def get_vader_sentiment(text):
    score = vader.polarity_scores(text)["compound"]
    if score > 0.1:
        return 1
    elif score < -0.1:
        return -1
    else:
        return 0

df["vader_pred"] = df["review_text"].apply(get_vader_sentiment)

"""## Flair – Deep Learning-Based Sentiment Classifier"""

!pip install flair
from flair.models import TextClassifier
from flair.data import Sentence

classifier = TextClassifier.load('en-sentiment')

def get_flair_sentiment(text):
    sentence = Sentence(text)
    classifier.predict(sentence)
    label = sentence.labels[0].value
    if label == "POSITIVE":
        return 1
    elif label == "NEGATIVE":
        return -1
    else:
        return 0

df["flair_pred"] = df["review_text"].apply(get_flair_sentiment)

"""## BERT (via Transformers) – Fine-Tuned Sentiment Model"""

!pip install transformers
from transformers import pipeline

sentiment_pipeline = pipeline("sentiment-analysis")

def get_bert_sentiment(text):
    result = sentiment_pipeline(text)[0]
    label = result['label']
    if label == "POSITIVE":
        return 1
    elif label == "NEGATIVE":
        return -1
    else:
        return 0

df["bert_pred"] = df["review_text"].apply(get_bert_sentiment)

"""# Report TextBlob / VADER / Flair / BERT

After building traditional machine learning models (Naive Bayes and Logistic Regression) using TF-IDF features, I was guided to explore how pretrained NLP models perform on the same sentiment classification task. These models are designed to work directly on raw text and often come with linguistic knowledge learned from massive corpora.

I tested four popular tools:

- TextBlob
- VADER
- Flair
- BERT

What I Learned About Each Model

1. TextBlob

- Type: Rule-based, built on top of NLTK and Pattern
- Strengths: Very easy to use, fast, and good for basic sentiment tasks
- Limitations: Struggles with nuanced or domain-specific language
- Insight: Best suited for quick prototyping or educational use

2. VADER (Valence Aware Dictionary and sEntiment Reasoner)

- Type: Lexicon and rule-based model, optimized for social media
- Strengths: Handles emojis, slang, and punctuation well
- Limitations: Limited to English and not context-aware
- Insight: Great for short, informal texts like tweets or reviews

3. Flair

- Type: Deep learning model using stacked embeddings
- Strengths: Captures contextual meaning better than rule-based models
- Limitations: Slower to run, requires more setup
- Insight: Performs well on longer reviews with subtle sentiment shifts

4. BERT (Bidirectional Encoder Representations from Transformers)

- Type: Transformer-based, pretrained on massive corpora
- Strengths: State-of-the-art performance, understands context deeply
- Limitations: Computationally heavy, slower inference
- Insight: Ideal for production-level sentiment analysis when accuracy matters most

Comparative Insights

When comparing all models:

- TextBlob and VADER were fast and surprisingly accurate on clear-cut reviews.
- Flair and BERT provided more nuanced predictions, especially on mixed or ambiguous feedback.
- Traditional models performed perfectly on my dataset, but may have benefited from overly clean or predictable data.

Reflections

Exploring these models helped me understand the trade-offs between simplicity and power in NLP:

- Rule-based models are fast and interpretable but limited in depth.
- Deep learning models are powerful but require more resources and care.

I now appreciate the importance of choosing the right tool based on the task, data quality, and deployment constraints.

# Compare All Models
"""

from sklearn.metrics import classification_report

# Traditional Models
print("Naive Bayes:")
print(classification_report(y_test, nb_preds))

print("Logistic Regression:")
print(classification_report(y_test, logreg_preds))

# Pretrained Models
print("TextBlob:")
print(classification_report(df["sentiment_label"], df["textblob_pred"]))

print("VADER:")
print(classification_report(df["sentiment_label"], df["vader_pred"]))

print("Flair:")
print(classification_report(df["sentiment_label"], df["flair_pred"]))

print("BERT:")
print(classification_report(df["sentiment_label"], df["bert_pred"]))

"""# Confusion Matrix for Pretrained Models

"""

# TextBlob
textblob_cm = confusion_matrix(df["sentiment_label"], df["textblob_pred"])

plt.figure(figsize=(6,4))
sns.heatmap(textblob_cm, annot=True, fmt='d', cmap='Purples', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])
plt.title("TextBlob – Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Vader
vader_cm = confusion_matrix(df["sentiment_label"], df["vader_pred"])

plt.figure(figsize=(6,4))
sns.heatmap(vader_cm, annot=True, fmt='d', cmap='Oranges', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])
plt.title("VADER – Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Flair
flair_cm = confusion_matrix(df["sentiment_label"], df["flair_pred"])

plt.figure(figsize=(6,4))
sns.heatmap(flair_cm, annot=True, fmt='d', cmap='Reds', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])
plt.title("Flair – Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Bert
bert_cm = confusion_matrix(df["sentiment_label"], df["bert_pred"])

plt.figure(figsize=(6,4))
sns.heatmap(bert_cm, annot=True, fmt='d', cmap='BuGn', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])
plt.title("BERT – Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""# Confusion Matrix for Pretrained Models - Report

## Comparative Analysis: Confusion Matrices of TextBlob, VADER, Flair, and BERT

| **Model**   | **Neutral Accuracy** | **Positive Accuracy** | **Negative Accuracy** | **Neutral Misclassification Pattern**     | **Notes**                                |
|-------------|----------------------|------------------------|------------------------|--------------------------------------------|-------------------------------------------|
| **TextBlob** | Very poor (18%)      | Perfect (100%)         | Strong (80%)           | Mostly misclassified as positive           | Rule-based; lacks nuance                  |
| **VADER**    | Moderate (25%)       | Strong (92%)           | Decent (76%)           | Mostly misclassified as positive           | Lexicon-based; positivity bias            |
| **Flair**    | None (0%)            | Perfect (100%)         | Excellent (99%)        | Split between negative and positive        | Deep learning; ignores neutral            |
| **BERT**     | None (0%)            | Perfect (100%)         | Perfect (100%)         | Split between negative and positive        | Deep learning; binary bias                |

---

# Final Analysis: Comparing Sentiment Models

##  Comparative Performance Summary

###  Traditional Models: Naive Bayes & Logistic Regression

- **Accuracy**: 100%  
- **Precision / Recall / F1-score**: Perfect across all classes  
- **Observation**:  
  These results are unusually high — likely due to clean, well-separated data. While impressive, they may not generalize to noisier or more ambiguous reviews. Caution is advised when applying these models to real-world datasets.

---

###  Pretrained Sentiment Models

| **Model**   | **Accuracy** | **Strengths**                          | **Limitations**                                 |
|-------------|--------------|----------------------------------------|-------------------------------------------------|
| **TextBlob** | 67%          | Fast, easy to use                      | Weak on neutral class, oversimplifies sentiment |
| **VADER**    | 67%          | Handles informal language well         | Struggles with nuanced sentiment                |
| **Flair**    | 67%          | Strong recall for positive/negative    | Completely misses neutral predictions           |
| **BERT**     | 67%          | Deep contextual understanding          | Ignores neutral class, slow inference           |

>  **Note**: The warning about undefined precision for class 0 (neutral) in Flair and BERT indicates that these models didn’t predict any samples as neutral — a sign of class imbalance or model bias.

---

###  Key Takeaways

- **Traditional models** excelled on this dataset, but may be overfitting or benefiting from ideal conditions.
- **TextBlob and VADER** are useful for quick sentiment tagging, but lack depth and struggle with neutrality.
- **Flair and BERT** show promise for nuanced understanding, yet require fine-tuning to handle all sentiment classes effectively.
- **Neutral sentiment** remains the hardest to classify — future work should prioritize improving this category.

---

# Model to Save
"""

import joblib

# Save TF-IDF vectorizer
joblib.dump(tfidf, 'tfidf_vectorizer.pkl')

# Save Naive Bayes model
joblib.dump(nb, 'naive_bayes_model.pkl')

# Save Logistic Regression model
joblib.dump(logreg, 'logistic_regression_model.pkl')

from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=5000)),
    ('clf', LogisticRegression(max_iter=1000))
])

pipeline.fit(X_train, y_train)
joblib.dump(pipeline, 'sentiment_pipeline.pkl')

"""# LSTM - fOR Sentiment Classification"""

# 1. Encode labels for categorical classification
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder

# Ensure labels are strings
df["sentiment_label"] = df["sentiment_label"].astype(str)

label_encoder = LabelEncoder()
df["sentiment_encoded"] = label_encoder.fit_transform(df["sentiment_label"])
y = to_categorical(df["sentiment_encoded"], num_classes=3)

# 2. Tokenize and pad sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

max_words = 10000
max_len = 120

tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(df["review_clean"])
sequences = tokenizer.texts_to_sequences(df["review_clean"])
X = pad_sequences(sequences, maxlen=max_len)

# 3. Train-validation split
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. EarlyStopping callback
from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(
    monitor='val_loss', # or val_accuraxy?
    patience=3,
    min_delta=1e-5,
    restore_best_weights=True
)

# 5. Build LSTM model (cuDNN-compatible)
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, SpatialDropout1D, BatchNormalization

vocab_size = min(max_words, len(tokenizer.word_index) + 1)

model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=128),  # Removed mask_zero
    SpatialDropout1D(0.3),
    Bidirectional(LSTM(64, return_sequences=False)),  # cuDNN-compatible
    BatchNormalization(),
    Dropout(0.5),
    Dense(3, activation='softmax')
])

model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

# 6. Train model
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=15,
    batch_size=32,
    callbacks=[early_stop],
    verbose=2
)

# 7. Evaluate model
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

y_pred_probs = model.predict(X_val)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = np.argmax(y_val, axis=1)

print("LSTM Classification Report:")
print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))

cm = confusion_matrix(y_true, y_pred)
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_).plot(xticks_rotation=45)
plt.title("LSTM Confusion Matrix")
plt.show()

model.save("lstm_sentiment_model.keras")

import joblib

joblib.dump(tokenizer, "lstm_tokenizer.pkl")
joblib.dump(label_encoder, "lstm_label_encoder.pkl")

import pickle

with open("lstm_training_history.pkl", "wb") as f:
    pickle.dump(history.history, f)

# from keras.models import load_model
# import joblib
# import pickle

# model = load_model("lstm_sentiment_model.keras")
# tokenizer = joblib.load("lstm_tokenizer.pkl")
# label_encoder = joblib.load("lstm_label_encoder.pkl")

# with open("lstm_training_history.pkl", "rb") as f:
#     history = pickle.load(f)

"""## Final Report: LSTM Model Performance and Training Insights

###  What I Learned

As part of my exploration into deep learning for text classification, I trained an LSTM model over 15 epochs. The results were both impressive and revealing, offering valuable lessons about model behavior, evaluation, and potential pitfalls.

---

### Training & Validation Metrics (Epochs 1–5)

| **Epoch** | **Training Accuracy** | **Training Loss** | **Validation Accuracy** | **Validation Loss** |
|-----------|------------------------|--------------------|--------------------------|----------------------|
| 1         | 0.9932                 | 0.0179             | 1.0000                   | 8.69e-05             |
| 2         | 1.0000                 | 2.21e-04           | 1.0000                   | 5.75e-07             |
| 3         | 1.0000                 | 6.70e-05           | 1.0000                   | 9.44e-07             |
| 4         | 1.0000                 | 6.03e-05           | 0.6713                   | 1.5528               |
| 5         | 0.9992                 | 0.0023             | 1.0000                   | 7.03e-10             |

>  **Observation**: The model reached near-perfect accuracy and extremely low loss within just a few epochs. However, the sudden drop in validation accuracy at epoch 4 (to 67.13%) suggests a possible data shuffle issue, overfitting, or a batch anomaly. This highlights the importance of monitoring both training and validation metrics continuously.

---

### Final Evaluation: Classification Report

| **Class** | **Precision** | **Recall** | **F1-Score** | **Support** |
|-----------|---------------|------------|--------------|-------------|
| -1        | 1.00          | 1.00       | 1.00         | 3325        |
| 0         | 1.00          | 1.00       | 1.00         | 3317        |
| 1         | 1.00          | 1.00       | 1.00         | 3358        |
| **Overall Accuracy** | **1.00** | — | — | **10,000** |

>  **Insight**: The model achieved perfect classification across all sentiment classes. While this is exciting, it may reflect ideal conditions, such as clean, balanced data or limited linguistic ambiguity, rather tHan true generalization.

---
"""