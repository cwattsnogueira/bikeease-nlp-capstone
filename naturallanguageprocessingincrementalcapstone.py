# -*- coding: utf-8 -*-
"""NaturalLanguageProcessingIncrementalCapstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N0cHR4SzFHh9n8htYrGDL8bRrleVffGc

# Natural Language Processing: Incremental Capstone

Input dataset:
https://drive.google.com/drive/folders/13-g3jxhPR0btN_s77KbcempcFXZ0RoqT

Carllos Watts-Nogueira

Due Aug 23 by 12:59am

Project Overview

The goal is to analyze thousands of customer reviews collected across platforms to:
- Automatically classify review sentiments (positive, neutral, negative)
- Extract underlying themes using topic modeling
- Help BikeEase respond better to user feedback and improve service

This transitions your role from purely predicting numbers (like rental counts) to understanding how people feel, using Natural Language Processing (NLP).

# Installing & importing packages
"""

# Install required packages (only once; skip if already installed)
# NLTK (Natural Language Toolkit) is a Python library for teaching and working with human language data, offering tools for tokenization, tagging, and parsing.
!pip install nltk

# Import necessary packages
import nltk
from nltk.tokenize import sent_tokenize

# import pandas
import pandas as pd

# Import Required Libraries
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

"""# Download necessary models from nltk"""

# Download required NLTK data
nltk.download('punkt')  # Sentence tokenizer
nltk.download('words')  # English word list
nltk.download('stopwords')  # Common stopwords
nltk.download('wordnet')  # WordNet lexical database
nltk.download('punkt_tab') # Download punkt_tab resource

"""# Import the necessary corpus"""

# Import WordNet, a lexical database used for lemmatization and semantic analysis
from nltk.corpus import wordnet

# Import a list of common English stopwords (e.g., "the", "and", "is") used for filtering out non-informative words
from nltk.corpus import stopwords

"""# Load your data and preprocess it"""

# Load dataset
df = pd.read_csv("bike_rental_reviews.csv")

df.head()

df.isnull().sum()

df.isna().sum()

"""# Change to lowercase / Removing punctuation / Word tokenization / Lemmatization / Remove stopwords"""

import re                # Import the regular expressions module for pattern-based text cleaning (e.g., removing punctuation) (regular expressions)
import string            # Import the string module to access common string constants like punctuation (punctuation and character sets)
from nltk.stem import WordNetLemmatizer  # lemmatizer class

# Define preprocessing a cleaning function --> clean_my_df
# Take the steps below and put them into a cleaning function, e.g. clean_my_df.
def clean_my_df(text):
    text = text.lower()
    text = re.sub(f"[{re.escape(string.punctuation)}]", "", text) #Removing punctuation
    tokens = nltk.word_tokenize(text) #Word tokenization
    lemmatizer = WordNetLemmatizer() #Lemmatization
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    stop_words = set(stopwords.words('english')) #Remove stopwords
    cleaned = [word for word in tokens if word not in stop_words]
    return " ".join(cleaned)

# Apply cleaning --> review_clean
# Apply your cleaning function to the DataFrame to create a new column, e.g. review_clean.
df["review_clean"] = df["review_text"].apply(clean_my_df)

# Encode sentiment labels
# Encode your sentiment labels into a new column sentiment_label.
sentiment_map = {'positive': 1, 'neutral': 0, 'negative': -1}
df["sentiment_label"] = df["sentiment"].map(sentiment_map)

df.head()

"""# Sentiment Analysis with TF-IDF, Naive Bayes, and Logistic Regression"""

# --------------------------------------------
# Final Step: Sentiment Analysis with TF-IDF, Naive Bayes, and Logistic Regression
# --------------------------------------------

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score

# Replace these with your actual column names!
text_column = df["review_clean"] #"your_clean_text"  # Column containing preprocessed review text
label_column = df["sentiment_label"] #"sentiment_label" # Column with sentiment labels (e.g., 1, 0, -1)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    df["review_clean"], df["sentiment_label"], test_size=0.2, random_state=42
)

# -------------------------------
# TF-IDF vectorization
# -------------------------------
tfidf = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

# -------------------------------
# Naive Bayes Model
# -------------------------------
nb = MultinomialNB()
nb.fit(X_train_tfidf, y_train)
nb_preds = nb.predict(X_test_tfidf)

print("Naive Bayes Performance:")
print("Accuracy:", accuracy_score(y_test, nb_preds))
print(classification_report(y_test, nb_preds))

# -------------------------------
# Logistic Regression Model
# -------------------------------
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train_tfidf, y_train)
logreg_preds = logreg.predict(X_test_tfidf)

print("Logistic Regression Performance:")
print("Accuracy:", accuracy_score(y_test, logreg_preds))
print(classification_report(y_test, logreg_preds))

# Confusion Matrix for Naive Bayes
nb_cm = confusion_matrix(y_test, nb_preds)

plt.figure(figsize=(6,4))
sns.heatmap(nb_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])
plt.title("Naive Bayes – Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Confusion Matrix for Logistic Regression
logreg_cm = confusion_matrix(y_test, logreg_preds)

plt.figure(figsize=(6,4))
sns.heatmap(logreg_cm, annot=True, fmt='d', cmap='Greens', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])
plt.title("Logistic Regression – Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""# Confusion Matrix for Logistic Regression / Naive Bayes - Report

Confusion Matrix Comparison

| Model | Negative | Neutral | Positive | Misclassifications |

| Naive Bayes | 3325 | 3317 | 3358 | 0 |

| Logistic Regression | 3325 | 3317 | 3358 | 0 |

- Both models achieved 100% accuracy.
- No misclassifications in either case.
- Identical performance across all sentiment categories.

Insights

- Data quality: The good results suggest the dataset is clean, balanced, and possibly too easy for traditional models.
- Model behavior: Logistic Regression and Naive Bayes both excelled, likely due to strong feature separation from TF-IDF vectorization.


**Risk of overfitting**?: Such perfect results may suggest the model is overfitting or the dataset lacks complexity.

# Final Report

Preprocessing Pipeline

To clean the review text, I applied the following steps using NLTK:

- Lowercasing all text
- Removing punctuation using re and string modules
- Tokenization with nltk.word_tokenize
- Lemmatization with WordNetLemmatizer
- Removal of English stopwords

This pipeline was encapsulated in a function called clean_my_df, and the output was stored in a new column called review_clean.

Label Encoding

Original sentiment labels were mapped to numeric values:

- positive → 1
- neutral → 0
- negative → -1

This allowed the models to treat the problem as a multiclass classification task.

Feature Extraction with TF-IDF

Using TfidfVectorizer, I transformed the preprocessed text into numerical features. The max_features parameter was set to 5000 to retain the most informative terms while controlling dimensionality.

Models Used

1. Multinomial Naive Bayes
- Fast and efficient for text classification
- Assumes conditional independence between words

2. Logistic Regression
- Linear model capable of handling multiclass problems
- Used max_iter=1000 to ensure convergence

Model Evaluation

Both models achieved 100% accuracy, precision, recall, and F1 scores across all classes.

Accuracy: 1.00

Classification Report:

-1: Precision = 1.00, Recall = 1.00, F1 = 1.00  
 0: Precision = 1.00, Recall = 1.00, F1 = 1.00  
 1: Precision = 1.00, Recall = 1.00, F1 = 1.00

While these results may seem too good to be true, I plan to investigate potential causes such as data leakage or overly simplistic text-label alignment.

Reflections

This project strengthened my understanding of:

- Text preprocessing pipelines
- Feature engineering with TF-IDF
- Building and evaluating classification models
- The importance of sanity-checking “good” results

# Add a BERT, Text Blob, Vader, or Flair model here and compare to your above results!

## TextBlob – Lightweight and Easy
"""

from textblob import TextBlob

def get_textblob_sentiment(text):
    polarity = TextBlob(text).sentiment.polarity
    if polarity > 0.1:
        return 1  # positive
    elif polarity < -0.1:
        return -1  # negative
    else:
        return 0  # neutral

df["textblob_pred"] = df["review_text"].apply(get_textblob_sentiment)

"""## VADER – Rule-Based, Great for Social Text"""

from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

vader = SentimentIntensityAnalyzer()

def get_vader_sentiment(text):
    score = vader.polarity_scores(text)["compound"]
    if score > 0.1:
        return 1
    elif score < -0.1:
        return -1
    else:
        return 0

df["vader_pred"] = df["review_text"].apply(get_vader_sentiment)

"""## Flair – Deep Learning-Based Sentiment Classifier"""

!pip install flair
from flair.models import TextClassifier
from flair.data import Sentence

classifier = TextClassifier.load('en-sentiment')

def get_flair_sentiment(text):
    sentence = Sentence(text)
    classifier.predict(sentence)
    label = sentence.labels[0].value
    if label == "POSITIVE":
        return 1
    elif label == "NEGATIVE":
        return -1
    else:
        return 0

df["flair_pred"] = df["review_text"].apply(get_flair_sentiment)

"""## BERT (via Transformers) – Fine-Tuned Sentiment Model"""

!pip install transformers
from transformers import pipeline

sentiment_pipeline = pipeline("sentiment-analysis")

def get_bert_sentiment(text):
    result = sentiment_pipeline(text)[0]
    label = result['label']
    if label == "POSITIVE":
        return 1
    elif label == "NEGATIVE":
        return -1
    else:
        return 0

df["bert_pred"] = df["review_text"].apply(get_bert_sentiment)

"""# Report TextBlob / VADER / Flair / BERT

After building traditional machine learning models (Naive Bayes and Logistic Regression) using TF-IDF features, I was guided to explore how pretrained NLP models perform on the same sentiment classification task. These models are designed to work directly on raw text and often come with linguistic knowledge learned from massive corpora.

I tested four popular tools:

- TextBlob
- VADER
- Flair
- BERT

What I Learned About Each Model

1. TextBlob

- Type: Rule-based, built on top of NLTK and Pattern
- Strengths: Very easy to use, fast, and good for basic sentiment tasks
- Limitations: Struggles with nuanced or domain-specific language
- Insight: Best suited for quick prototyping or educational use

2. VADER (Valence Aware Dictionary and sEntiment Reasoner)

- Type: Lexicon and rule-based model, optimized for social media
- Strengths: Handles emojis, slang, and punctuation well
- Limitations: Limited to English and not context-aware
- Insight: Great for short, informal texts like tweets or reviews

3. Flair

- Type: Deep learning model using stacked embeddings
- Strengths: Captures contextual meaning better than rule-based models
- Limitations: Slower to run, requires more setup
- Insight: Performs well on longer reviews with subtle sentiment shifts

4. BERT (Bidirectional Encoder Representations from Transformers)

- Type: Transformer-based, pretrained on massive corpora
- Strengths: State-of-the-art performance, understands context deeply
- Limitations: Computationally heavy, slower inference
- Insight: Ideal for production-level sentiment analysis when accuracy matters most

Comparative Insights

When comparing all models:

- TextBlob and VADER were fast and surprisingly accurate on clear-cut reviews.
- Flair and BERT provided more nuanced predictions, especially on mixed or ambiguous feedback.
- Traditional models performed perfectly on my dataset, but may have benefited from overly clean or predictable data.

Reflections

Exploring these models helped me understand the trade-offs between simplicity and power in NLP:

- Rule-based models are fast and interpretable but limited in depth.
- Deep learning models are powerful but require more resources and care.

I now appreciate the importance of choosing the right tool based on the task, data quality, and deployment constraints.

# Compare All Models
"""

from sklearn.metrics import classification_report

# Traditional Models
print("Naive Bayes:")
print(classification_report(y_test, nb_preds))

print("Logistic Regression:")
print(classification_report(y_test, logreg_preds))

# Pretrained Models
print("TextBlob:")
print(classification_report(df["sentiment_label"], df["textblob_pred"]))

print("VADER:")
print(classification_report(df["sentiment_label"], df["vader_pred"]))

print("Flair:")
print(classification_report(df["sentiment_label"], df["flair_pred"]))

print("BERT:")
print(classification_report(df["sentiment_label"], df["bert_pred"]))

"""# Confusion Matrix for Pretrained Models

"""

# TextBlob
textblob_cm = confusion_matrix(df["sentiment_label"], df["textblob_pred"])

plt.figure(figsize=(6,4))
sns.heatmap(textblob_cm, annot=True, fmt='d', cmap='Purples', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])
plt.title("TextBlob – Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Vader
vader_cm = confusion_matrix(df["sentiment_label"], df["vader_pred"])

plt.figure(figsize=(6,4))
sns.heatmap(vader_cm, annot=True, fmt='d', cmap='Oranges', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])
plt.title("VADER – Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Flair
flair_cm = confusion_matrix(df["sentiment_label"], df["flair_pred"])

plt.figure(figsize=(6,4))
sns.heatmap(flair_cm, annot=True, fmt='d', cmap='Reds', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])
plt.title("Flair – Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Bert
bert_cm = confusion_matrix(df["sentiment_label"], df["bert_pred"])

plt.figure(figsize=(6,4))
sns.heatmap(bert_cm, annot=True, fmt='d', cmap='BuGn', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])
plt.title("BERT – Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""# Confusion Matrix for Pretrained Models - Report

Comparative Analysis: Confusion Matrices of TextBlob, VADER, Flair, and BERT

| Model | Neutral Accuracy | Positive Accuracy | Negative Accuracy | Neutral Misclassification Pattern | Notes |

| TextBlob | Very poor (18%) | Perfect (100%) | Strong (80%) | Mostly misclassified as positive | Rule-based; lacks nuance |

| VADER | Moderate (25%) | Strong (92%) | Decent (76%) | Mostly misclassified as positive | Lexicon-based; positivity bias |

| Flair | None (0%) | Perfect (100%) | Excellent (99%) | Split between negative and positive | Deep learning; ignores neutral |

| BERT | None (0%) | Perfect (100%) | Perfect (100%) | Split between negative and positive | Deep learning; binary bias |

# Final Analysis: Comparing Sentiment Models

Traditional Models (Naive Bayes & Logistic Regression)

- Accuracy: 100%
- Precision/Recall/F1: Perfect across all classes
- Observation: These results are unusually high, likely due to clean, well-separated data. While impressive, they may not generalize to noisier or more ambiguous reviews.

Pretrained Models

| Model | Accuracy | Strengths | Limitations |

| TextBlob | 67% | Fast, easy to use | Weak on neutral class, oversimplifies |

| VADER | 67% | Handles informal language well | Struggles with nuanced sentiment |

| Flair | 67% | Strong recall for positive/negative | Completely misses neutral predictions |

| BERT | 67% | Deep contextual understanding | Ignores neutral class, slow inference |

Note: The warning about undefined precision for class 0 (neutral) in Flair and BERT means those models didn’t predict any samples as neutral, a sign of class imbalance or model bias.

Key Takeaways

- Traditional models excelled on this dataset, but may be overfitting or benefiting from ideal conditions.
- TextBlob and VADER are great for quick sentiment tagging but lack depth.
- Flair and BERT show potential for nuanced understanding but need fine-tuning to handle all classes effectively.
- Neutral sentiment is consistently the hardest to classify, future work should focus on improving this category.

# Model to Save
"""

import joblib

# Save TF-IDF vectorizer
joblib.dump(tfidf, 'tfidf_vectorizer.pkl')

# Save Naive Bayes model
joblib.dump(nb, 'naive_bayes_model.pkl')

# Save Logistic Regression model
joblib.dump(logreg, 'logistic_regression_model.pkl')

from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=5000)),
    ('clf', LogisticRegression(max_iter=1000))
])

pipeline.fit(X_train, y_train)
joblib.dump(pipeline, 'sentiment_pipeline.pkl')